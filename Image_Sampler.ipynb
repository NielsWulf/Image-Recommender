{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from PIL import Image\n",
    "from typing import List, Tuple, Dict, Any, Generator \n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"image_data\"\n",
    "database_path = \"image_metadata.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_files_with_metadata(root_dir: str, batch_size: int = 1000) -> Generator[List[Dict], None, None]:\n",
    "    \"\"\"\n",
    "    Recursively find all image files in the directory and return basic metadata in batches.\n",
    "    \n",
    "    Args:\n",
    "    - root_dir (str): The root directory to start searching for images.\n",
    "    - batch_size (int): Number of metadata entries per batch.\n",
    "    \n",
    "    Yields:\n",
    "    - List[Dict]: A batch of metadata dictionaries.\n",
    "    \"\"\"\n",
    "    metadata_batch = []\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                full_path = os.path.join(subdir, file)\n",
    "                image = cv2.imread(full_path)\n",
    "                if image is not None:\n",
    "                    height, width, _ = image.shape\n",
    "                    metadata_batch.append({\n",
    "                        'file_name': file,\n",
    "                        'file_path': full_path,\n",
    "                        'directory': subdir,\n",
    "                        'width': width,\n",
    "                        'height': height,\n",
    "                    })\n",
    "                    if len(metadata_batch) == batch_size:\n",
    "                        yield metadata_batch\n",
    "                        metadata_batch = []\n",
    "    if metadata_batch:\n",
    "        yield metadata_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|█████████▉| 444668/444682 [7:39:08<00:00, 16.14image/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 1, 'file_name': '000000000034.jpg', 'file_path': 'image_data\\\\coco2017_train\\\\train2017\\\\000000000034.jpg', 'directory': 'image_data\\\\coco2017_train\\\\train2017', 'width': 640, 'height': 425}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def setup_database(db_path: str):\n",
    "    \"\"\"\n",
    "    Set up the SQLite database with the required schema.\n",
    "    \n",
    "    Args:\n",
    "    - db_path (str): Path to the SQLite database file.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Drop the existing table if it exists\n",
    "    cursor.execute('''\n",
    "    DROP TABLE IF EXISTS images\n",
    "    ''')\n",
    "    \n",
    "    # Create the new table with the updated schema\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE images (\n",
    "        image_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        file_name TEXT NOT NULL,\n",
    "        file_path TEXT NOT NULL,\n",
    "        directory TEXT NOT NULL,\n",
    "        width INTEGER NOT NULL,\n",
    "        height INTEGER NOT NULL\n",
    "    )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def insert_metadata_batch(conn: sqlite3.Connection, metadata_batch: List[Dict]):\n",
    "    \"\"\"\n",
    "    Insert a batch of image metadata into the database.\n",
    "    \n",
    "    Args:\n",
    "    - conn (sqlite3.Connection): SQLite connection object.\n",
    "    - metadata_batch (List[Dict]): List of metadata dictionaries.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.executemany('''\n",
    "    INSERT INTO images (file_name, file_path, directory, width, height)\n",
    "    VALUES (:file_name, :file_path, :directory, :width, :height)\n",
    "    ''', metadata_batch)\n",
    "    conn.commit()\n",
    "\n",
    "def retrieve_metadata(conn: sqlite3.Connection, image_id: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Retrieve image metadata from the database.\n",
    "    \n",
    "    Args:\n",
    "    - conn (sqlite3.Connection): SQLite connection object.\n",
    "    - image_id (int): ID of the image to retrieve metadata for.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionary containing image metadata.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "    SELECT image_id, file_name, file_path, directory, width, height FROM images WHERE image_id = ?\n",
    "    ''', (image_id,))\n",
    "    row = cursor.fetchone()\n",
    "    if row:\n",
    "        return {\n",
    "            'image_id': row[0],\n",
    "            'file_name': row[1],\n",
    "            'file_path': row[2],\n",
    "            'directory': row[3],\n",
    "            'width': row[4],\n",
    "            'height': row[5]\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    root_directory = \"image_data\"\n",
    "    database_path = \"image_metadata.db\"\n",
    "    \n",
    "    # Set up database\n",
    "    setup_database(database_path)\n",
    "\n",
    "    # Open a connection to the database\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    \n",
    "    # Initialize tqdm progress bar\n",
    "    total_images = sum([len(files) for r, d, files in os.walk(root_directory) if files])\n",
    "    progress_bar = tqdm(total=total_images, desc=\"Processing Images\", unit=\"image\")\n",
    "\n",
    "    # Collect and insert metadata in batches\n",
    "    for metadata_batch in find_image_files_with_metadata(root_directory):\n",
    "        insert_metadata_batch(conn, metadata_batch)\n",
    "        progress_bar.update(len(metadata_batch))\n",
    "    \n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "    \n",
    "    # Retrieve metadata for testing (this line just retrieves one image's metadata as a test)\n",
    "    image_metadata = retrieve_metadata(conn, 1)\n",
    "    print(image_metadata)\n",
    "    \n",
    "    # Close the database connection\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_in_batches(image_paths: List[str], batch_size: int = 100) -> Generator[List[np.ndarray], None, None]:\n",
    "    \"\"\"Yield batches of images from the disk.\"\"\"\n",
    "    batch = []\n",
    "    for path in image_paths:\n",
    "        image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        if image is not None:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            batch.append(image)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "# Ensure metadata_df is defined\n",
    "if 'metadata_df' in globals():\n",
    "    # Get image paths from the metadata DataFrame\n",
    "    image_paths = metadata_df['file_path'].tolist()\n",
    "\n",
    "    # Load the first batch of images\n",
    "    image_batches = load_images_in_batches(image_paths, batch_size=10)\n",
    "    first_batch = next(image_batches)\n",
    "\n",
    "    # Display the first batch of images\n",
    "    for i, img in enumerate(first_batch):\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Image {i+1}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()  # Ensure each image is displayed\n",
    "else:\n",
    "    print(\"metadata_df is not defined. Please run the previous cell to collect basic metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image: np.ndarray, model: PCA) -> np.ndarray:\n",
    "    \"\"\"Extract features using PCA (as a placeholder for deep model).\"\"\"\n",
    "    image_flat = image.flatten().reshape(1, -1)\n",
    "    return model.transform(image_flat)\n",
    "\n",
    "def color_similarity(image1: np.ndarray, image2: np.ndarray) -> float:\n",
    "    \"\"\"Compute similarity based on color histograms.\"\"\"\n",
    "    hist1 = cv2.calcHist([image1], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "    hist2 = cv2.calcHist([image2], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "    return cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
    "\n",
    "def custom_similarity(image1: np.ndarray, image2: np.ndarray) -> float:\n",
    "    \"\"\"Compute texture similarity using Local Binary Patterns.\"\"\"\n",
    "    lbp1 = local_binary_pattern(cv2.cvtColor(image1, cv2.COLOR_RGB2GRAY), 24, 8, method='uniform')\n",
    "    lbp2 = local_binary_pattern(cv2.cvtColor(image2, cv2.COLOR_RGB2GRAY), 24, 8, method='uniform')\n",
    "    hist1, _ = np.histogram(lbp1.ravel(), bins=np.arange(0, 27), range=(0, 26))\n",
    "    hist2, _ = np.histogram(lbp2.ravel(), bins=np.arange(0, 27), range=(0, 26))\n",
    "    hist1 = hist1.astype('float32')\n",
    "    hist2 = hist2.astype('float32')\n",
    "    return cosine_similarity([hist1], [hist2])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_batch(batch: List[np.ndarray], reference_image: np.ndarray, model: PCA):\n",
    "    \"\"\"Process a batch of images and compute similarity metrics.\"\"\"\n",
    "    results = []\n",
    "    reference_features = extract_features(reference_image, model)\n",
    "    \n",
    "    for i, image in enumerate(batch):\n",
    "        print(f\"Processing image {i+1}/{len(batch)}\")  # Debug statement\n",
    "        color_sim = color_similarity(reference_image, image)\n",
    "        deep_sim = cosine_similarity(reference_features, extract_features(image, model))[0][0]\n",
    "        custom_sim = custom_similarity(reference_image, image)\n",
    "        results.append({\n",
    "            'color_similarity': color_sim,\n",
    "            'deep_learning_similarity': deep_sim,\n",
    "            'custom_similarity': custom_sim\n",
    "        })\n",
    "    \n",
    "    # Display results for the first few images in the batch\n",
    "    for i, result in enumerate(results[:5]):\n",
    "        print(f\"Image {i}:\")\n",
    "        print(f\"Color Similarity: {result['color_similarity']:.4f}\")\n",
    "        print(f\"Deep Learning Similarity: {result['deep_learning_similarity']:.4f}\")\n",
    "        print(f\"Custom Similarity: {result['custom_similarity']:.4f}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_learning_similarity(image1: np.ndarray, image2: np.ndarray, model: PCA) -> float:\n",
    "    \"\"\"Compute similarity using deep learning features.\"\"\"\n",
    "    feature1 = extract_features(image1, model)\n",
    "    feature2 = extract_features(image2, model)\n",
    "    return cosine_similarity(feature1, feature2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = PCA()\n",
    "def find_best_matches(input_image: np.ndarray, all_images: List[np.ndarray], num_matches: int = 5) -> List[Tuple[float, int]]:\n",
    "    \"\"\"Find the best matches for the input image from all images.\"\"\"\n",
    "    similarities = []\n",
    "    for idx, image in enumerate(all_images):\n",
    "        score = (\n",
    "            color_similarity(input_image, image) +\n",
    "            deep_learning_similarity(input_image, image, pca_model) +\n",
    "            custom_similarity(input_image, image)\n",
    "        )\n",
    "        similarities.append((score, idx))\n",
    "    similarities.sort(reverse=True, key=lambda x: x[0])\n",
    "    return similarities[:num_matches]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
